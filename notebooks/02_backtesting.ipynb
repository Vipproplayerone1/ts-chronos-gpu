{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting Analysis\n",
    "\n",
    "This notebook analyzes the results from rolling-origin backtesting on the validation set.\n",
    "\n",
    "## Contents\n",
    "1. Load backtesting results\n",
    "2. Model performance comparison\n",
    "3. Metrics by fold\n",
    "4. Error analysis by horizon\n",
    "5. Probabilistic forecast evaluation\n",
    "6. Statistical significance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from src.utils import load_metrics, load_yaml\n",
    "from src.plots import (\n",
    "    plot_metrics_by_fold,\n",
    "    plot_error_by_horizon\n",
    ")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Backtesting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load all model metrics\nmodels = ['seasonal_naive', 'ets', 'gradient_boosting', 'chronos']\n\nall_metrics = {}\nfor model in models:\n    try:\n        metrics = load_metrics(model, '../artifacts/metrics')\n        all_metrics[model] = metrics\n        print(f\"[OK] Loaded {model}\")\n    except Exception as e:\n        print(f\"[FAIL] Failed to load {model}: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics comparison table\n",
    "metrics_df = pd.DataFrame(all_metrics).T\n",
    "\n",
    "# Select key metrics\n",
    "key_metrics = ['mae_mean', 'rmse_mean', 'smape_mean', 'mase_mean']\n",
    "display_df = metrics_df[key_metrics].copy()\n",
    "\n",
    "# Sort by MASE\n",
    "display_df = display_df.sort_values('mase_mean')\n",
    "\n",
    "print(\"\\n=== Validation Performance (Mean across folds) ===\")\n",
    "print(display_df.to_string())\n",
    "print(\"\\nBest model (by MASE):\", display_df.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "metric_names = ['mae_mean', 'rmse_mean', 'smape_mean', 'mase_mean']\n",
    "titles = ['MAE', 'RMSE', 'sMAPE (%)', 'MASE']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metric_names, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    values = [all_metrics[model][metric] for model in models if metric in all_metrics[model]]\n",
    "    model_names = [model for model in models if metric in all_metrics[model]]\n",
    "    \n",
    "    ax.bar(model_names, values, edgecolor='black')\n",
    "    ax.set_title(f'{title} Comparison', fontweight='bold')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Fold-Level Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load backtesting predictions\nbacktest_dfs = {}\n\nfor model in models:\n    try:\n        df = pd.read_parquet(f'../artifacts/predictions/{model}_backtest.parquet')\n        backtest_dfs[model] = df\n        print(f\"[OK] Loaded {model} predictions: {len(df)} records\")\n    except Exception as e:\n        print(f\"[FAIL] Failed to load {model}: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Analysis by Horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MAE by horizon for each model\n",
    "error_by_horizon = {}\n",
    "\n",
    "for model, df in backtest_dfs.items():\n",
    "    horizon_errors = df.groupby('horizon').apply(\n",
    "        lambda x: np.mean(np.abs(x['y_true'] - x['y_pred']))\n",
    "    ).to_dict()\n",
    "    error_by_horizon[model] = horizon_errors\n",
    "\n",
    "# Plot\n",
    "plot_error_by_horizon(error_by_horizon, metric_name='MAE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load statistical test results\n",
    "try:\n",
    "    stats_df = pd.read_csv('../artifacts/metrics/statistical_tests.csv')\n",
    "    print(\"\\n=== Statistical Significance Tests ===\")\n",
    "    print(stats_df.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"Statistical tests not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Probabilistic Forecast Evaluation (Chronos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if quantile predictions are available\n",
    "if 'chronos' in backtest_dfs:\n",
    "    chronos_df = backtest_dfs['chronos']\n",
    "    \n",
    "    # Check for quantile columns\n",
    "    quantile_cols = [col for col in chronos_df.columns if col.startswith('q_')]\n",
    "    \n",
    "    if quantile_cols:\n",
    "        print(f\"\\nQuantile predictions available: {quantile_cols}\")\n",
    "        \n",
    "        # Compute coverage\n",
    "        if 'q_0.1' in chronos_df.columns and 'q_0.9' in chronos_df.columns:\n",
    "            coverage_80 = np.mean(\n",
    "                (chronos_df['y_true'] >= chronos_df['q_0.1']) & \n",
    "                (chronos_df['y_true'] <= chronos_df['q_0.9'])\n",
    "            )\n",
    "            print(f\"\\n80% Prediction Interval Coverage: {coverage_80*100:.2f}%\")\n",
    "            print(f\"  Expected: 80%\")\n",
    "            print(f\"  {'Well-calibrated!' if abs(coverage_80 - 0.8) < 0.05 else 'Needs calibration'}\")\n",
    "    else:\n",
    "        print(\"\\nNo quantile predictions found in backtest results\")\n",
    "else:\n",
    "    print(\"\\nChronos results not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings from backtesting:\n",
    "1. Best performing model (by MASE)\n",
    "2. Error increases with forecast horizon\n",
    "3. Statistical significance of model differences\n",
    "4. Calibration quality of probabilistic forecasts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}