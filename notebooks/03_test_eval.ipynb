{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set Evaluation\n",
    "\n",
    "Final evaluation of all models on the held-out test set.\n",
    "\n",
    "## Contents\n",
    "1. Load test results\n",
    "2. Final performance comparison\n",
    "3. Forecast visualizations\n",
    "4. Calibration analysis\n",
    "5. Residuals analysis\n",
    "6. Feature importance (Gradient Boosting)\n",
    "7. Final conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils import load_yaml\n",
    "from src.plots import (\n",
    "    plot_forecasts_with_history,\n",
    "    plot_calibration_curve,\n",
    "    plot_residuals_analysis\n",
    ")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results summary\n",
    "results = load_yaml('../artifacts/results_summary.yaml')\n",
    "\n",
    "print(\"Execution time:\", results['execution_time'])\n",
    "print(\"\\nData info:\")\n",
    "for key, value in results['data_info'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test metrics\n",
    "test_metrics = results['test_metrics']\n",
    "\n",
    "# Create comparison table\n",
    "test_df = pd.DataFrame(test_metrics).T\n",
    "\n",
    "# Select key metrics\n",
    "key_metrics = [col for col in test_df.columns if col in ['mae', 'rmse', 'smape', 'mase']]\n",
    "display_df = test_df[key_metrics].copy()\n",
    "\n",
    "# Sort by MASE\n",
    "if 'mase' in display_df.columns:\n",
    "    display_df = display_df.sort_values('mase')\n",
    "\n",
    "print(\"\\n=== Test Set Performance ===\")\n",
    "print(display_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare Validation vs Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare validation and test MASE\n",
    "val_metrics = results['validation_metrics']\n",
    "\n",
    "comparison_data = []\n",
    "for model in test_metrics.keys():\n",
    "    val_mase = val_metrics.get(model, {}).get('mase_mean', np.nan)\n",
    "    test_mase = test_metrics.get(model, {}).get('mase', np.nan)\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': model,\n",
    "        'Validation MASE': val_mase,\n",
    "        'Test MASE': test_mase,\n",
    "        'Difference': test_mase - val_mase\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n=== Validation vs Test MASE ===\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize validation vs test\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, comparison_df['Validation MASE'], width, label='Validation', edgecolor='black')\n",
    "ax.bar(x + width/2, comparison_df['Test MASE'], width, label='Test', edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('MASE')\n",
    "ax.set_title('Validation vs Test Performance', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Visualize Test Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if forecast plot exists\n",
    "forecast_plot = Path('../artifacts/figures/test_forecasts.png')\n",
    "\n",
    "if forecast_plot.exists():\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(filename=str(forecast_plot)))\n",
    "else:\n",
    "    print(\"Forecast plot not found. Run the pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calibration Analysis (Chronos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if calibration plot exists\n",
    "calibration_plot = Path('../artifacts/figures/calibration_curve.png')\n",
    "\n",
    "if calibration_plot.exists():\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(filename=str(calibration_plot)))\n",
    "else:\n",
    "    print(\"Calibration plot not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model rankings\n",
    "rankings = results.get('model_rankings', {})\n",
    "\n",
    "print(\"\\n=== Model Rankings (by MASE) ===\")\n",
    "for rank, (model, score) in enumerate(sorted(rankings.items(), key=lambda x: x[1]), 1):\n",
    "    print(f\"{rank}. {model}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Library Versions (Reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display library versions\n",
    "versions = results.get('library_versions', {})\n",
    "\n",
    "print(\"\\n=== Library Versions ===\")\n",
    "for lib, version in versions.items():\n",
    "    print(f\"{lib}: {version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Additional Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for seasonality decomposition\n",
    "seasonality_plot = Path('../artifacts/figures/seasonality_decomposition.png')\n",
    "\n",
    "if seasonality_plot.exists():\n",
    "    print(\"\\n=== Seasonality Decomposition ===\")\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(filename=str(seasonality_plot)))\n",
    "else:\n",
    "    print(\"Seasonality plot not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "1. **Best Model**: [Will be determined after running pipeline]\n",
    "2. **Validation vs Test**: [Consistency analysis]\n",
    "3. **Chronos Performance**: [Zero-shot vs baselines]\n",
    "4. **Calibration**: [Prediction interval quality]\n",
    "\n",
    "### Next Steps:\n",
    "1. Document findings in report\n",
    "2. Create presentation slides\n",
    "3. Write model card\n",
    "4. Consider model deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
